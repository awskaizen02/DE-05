{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28b8d8a2-53d9-43f0-b9c5-63eb5df804e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5d6c098-3978-4ce6-a69f-3ccff8bf78ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://www.databricks.com/glossary/what-are-dataframes\n",
    "\n",
    "# DataFrame is a two dimenstional table consists of rows and columns like spreadheet\n",
    "# Each column is having respected column name\n",
    "# Each row will be determined by the row number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fa26bb-6bea-4a9d-bc18-d7c0125accca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n| ID|        Name|\n+---+------------+\n|100|      Thilak|\n|101|        Arun|\n|102|Hemachandran|\n|103|     mohamed|\n|104|        irin|\n|105|     Manisah|\n+---+------------+\n\nOut[4]: pyspark.sql.dataframe.DataFrame"
     ]
    }
   ],
   "source": [
    "# create a dataframe from a static list\n",
    "list01 = [(100,'Thilak'),(101,'Arun'),(102,'Hemachandran'),(103,'mohamed'),(104,'irin'),(105,'Manisah')]\n",
    "col_name = ['ID','Name']\n",
    "df = spark.createDataFrame(data = list01, schema = col_name)\n",
    "df.show()\n",
    "type(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8086e64d-9955-47c6-86e4-c5069d6c5455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86b11e69-4607-43b3-b76d-2c4d649ee3e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-----+\n|emp_id|  name|salary|  loc|\n+------+------+------+-----+\n|     1|manish| 20000|india|\n|     2|  hari|  5000|   UK|\n|     3| rahul| 10000|india|\n|     4|   nil| 20000|india|\n|     5|   sil| 24000|   UK|\n|     6|  neha| 17000|   UK|\n+------+------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Create a dataframfrom csv file load in DBFS\n",
    "\n",
    "file_path = '/FileStore/emp_2.csv'\n",
    "df = spark.read.format(\"csv\").option(\"header\",\"True\").load(file_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da0eb47-b412-4127-b5d1-14b098c80666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n| ID|NAME| DEPT|\n+---+----+-----+\n|101|AJAY|SALES|\n|102| RAJ|   HR|\n+---+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# crate a datafram from txt file locatd in DBFS\n",
    "file_path = \"/FileStore/PIPETEST.txt\"\n",
    "df = spark.read.format(\"csv\").option(\"header\",\"True\").option(\"sep\",\"|\").load(file_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41ce3504-fcf6-4e94-ad6a-c9b1902fe365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n| ID|NAME| DEPT|\n+---+----+-----+\n|101|AJAY|SALES|\n|102| RAJ|   HR|\n+---+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#write the above script with options instead of options\n",
    "file_path = \"/FileStore/PIPETEST.txt\"\n",
    "df = spark.read.format(\"csv\").options(header = True, sep = '|').load(file_path)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b81324-cb4a-425f-8975-f05a504818fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-------------+-----------+---------+\n|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|SALARY|DEPARTMENT_ID|LOCATION_ID|HIRE_DATE|\n+-----------+----------+---------+------+-------------+-----------+---------+\n|        101|    Donald|     null|  2600|           10|       1701|21-Jun-07|\n|        102|   Douglas|    Grant|  2600|           20|       1702|13-Jan-08|\n|        103|  Jennifer|   Whalen|  4400|           30|       1703|17-Sep-03|\n+-----------+----------+---------+------+-------------+-----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#read a file from a parquet file\n",
    "test_path = '/FileStore/emp1.parquet'\n",
    "df = spark.read.format(\"parquet\").load(test_path)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a1b7fc2-752b-4e5f-8189-8d9976a06b8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: [MountInfo(mountPoint='/databricks-datasets', source='databricks-datasets', encryptionType=''),\n MountInfo(mountPoint='/databricks/mlflow-tracking', source='databricks/mlflow-tracking', encryptionType='sse-s3'),\n MountInfo(mountPoint='/databricks-results', source='databricks-results', encryptionType='sse-s3'),\n MountInfo(mountPoint='/databricks/mlflow-registry', source='databricks/mlflow-registry', encryptionType='sse-s3'),\n MountInfo(mountPoint='/', source='DatabricksRoot', encryptionType='sse-s3')]"
     ]
    }
   ],
   "source": [
    "# check the list of mount ponts\n",
    "dbutils.fs.mounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb7ffe44-1124-4043-81d7-3f85573e3795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## mounint ADLS and SQL into DBframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "966208e7-6e48-4a51-84c1-af790dff5b9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3104fee9-3b3b-429d-9f02-05e591bf0044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://www.databricks.com/glossary/what-is-data-transformation\n",
    "\n",
    "# Data transformation is the process of taking raw data that has been extracted from data sources and turning it into usable datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afab1c5a-9869-44ca-b6e5-199da2ac377d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Select required columns\n",
    "changing the data type\n",
    "Adding calculated columns\n",
    "renaming columns\n",
    "filtering\n",
    "string and data formatting\n",
    "Aggregations\n",
    "remove nulls and duplicates\n",
    "joins\n",
    "unions\n",
    "rank,dense rank, getting lastrow an net row values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61c316eb-6505-4056-b8e1-ec5d7aef0376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+------------+-----+-----+\n|    firstname|         lastname|     country|state|  sal|\n+-------------+-----------------+------------+-----+-----+\n|         raja|           pushpa|         USA|     |30000|\n|        priya|           pushpa|         USA|     |29900|\n|      Karthik|             Subu|         USA|   CA| 6000|\n|        James|Smith            |         USA|   FL|20000|\n|       Martin|            Jones|         USA|   CA| 3000|\n|          Sam| Anderson        |          UK|  LND| 8000|\n|        Maria|          Patrick|          UK|  MCR| 7000|\n|        Robet|            Bevon|          UK|  LND| 3500|\n|        Maria|         Anderson|          UK|  MCR| 3000|\n+-------------+-----------------+------------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe from a static list\n",
    "# syntax: spark.createDataFrame(data = <list> , schema = <schemalist>)\n",
    "\n",
    "staticlist = [(\" raja\", \"pushpa\", \"USA\",\"\",30000),\n",
    "            (\"        priya\", \"     pushpa\", \"USA\",\"\",29900),\n",
    "            (\" Karthik\",       \"Subu\", \"USA\",\"CA\",6000),\n",
    "            (\" James\", \"Smith            \", \"USA\",\"FL\",20000),\n",
    "            (\"Martin\", \"Jones\", \"         USA\",\"CA\",3000),\n",
    "            (\"Sam\", \"Anderson        \", \"UK\",\"LND\",8000),\n",
    "            (\"      Maria\", \"       Patrick\", \"UK\",\"MCR\",7000),\n",
    "            (\"Robet\", \"Bevon\", \"UK\",\"LND\",3500),\n",
    "            (\"Maria\", \"         Anderson\", \"UK\",\"MCR\",3000)\n",
    "            ]\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\",\"sal\"]            \n",
    "df = spark.createDataFrame( data = staticlist, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a10625b0-1421-4931-9a9e-e5ca3b23fe8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- country: string (nullable = true)\n |-- state: string (nullable = true)\n |-- sal: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# check the datatype of any variable\n",
    "\n",
    "type(df)\n",
    "\n",
    "#check the schema of the dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec20e49-2329-4b45-a0e1-74288974ca91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/sql-ref-datatypes.html\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html\n",
    "\n",
    "# create a new schema for a dataframe using the StructType and StructField\n",
    "\n",
    "from pyspark.sql.types import * \n",
    "\n",
    "StructSchema = StructType([\n",
    "                            StructField('FirstName',StringType(),False),\n",
    "                            StructField('LastName',StringType(),True),\n",
    "                            StructField('Country',StringType(),False),\n",
    "                            StructField('State',StringType(),False),\n",
    "                            StructField('sal',StringType(),False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0bf50e5-3b13-4b8d-ad33-6e16695bf67f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+------------+-----+-----+\n|    FirstName|LastName|     Country|State|  sal|\n+-------------+--------+------------+-----+-----+\n|         raja|  pushpa|         USA|     |30000|\n|        priya|  pushpa|         USA|     |29900|\n|      Karthik|    Subu|         USA|   CA| 6000|\n|        James|   Smith|         USA|   FL|20000|\n|       Martin|   Jones|         USA|   CA| 3000|\n|          Sam|Anderson|          UK|  LND| 8000|\n|        Maria| Patrick|          UK|  MCR| 7000|\n|        Robet|   Bevon|          UK|  LND| 3500|\n|        Maria|Anderson|          UK|  MCR| 3000|\n+-------------+--------+------------+-----+-----+\n\nroot\n |-- FirstName: string (nullable = false)\n |-- LastName: string (nullable = true)\n |-- Country: string (nullable = false)\n |-- State: string (nullable = false)\n |-- sal: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "# create the datafreame using the new struct schema\n",
    "df = spark.createDataFrame(data = staticlist, schema= StructSchema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c909ac54-bcbe-46fc-a7d7-26b497f30f34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# most used pyspark libaries\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26943335-3739-4000-869c-e320cef42607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|  sal|\n+-----+\n|30000|\n|29900|\n| 6000|\n|20000|\n| 3000|\n| 8000|\n| 7000|\n| 3500|\n| 3000|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Select specific columns from a dataframe using the \"column name\" selection\n",
    "# usinsg name in inverted columns is not case sensitive\n",
    "\n",
    "df1 = df.select (\"sal\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0337c96b-2711-4b6c-a638-1e46206f60b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n|    firstname|Lastname|\n+-------------+--------+\n|         raja|  pushpa|\n|        priya|  pushpa|\n|      Karthik|    Subu|\n|        James|   Smith|\n|       Martin|   Jones|\n|          Sam|Anderson|\n|        Maria| Patrick|\n|        Robet|   Bevon|\n|        Maria|Anderson|\n+-------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select(\"firstname\",\"Lastname\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "537e6666-081f-4d2b-8c15-37e4f56fec1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n|    FirstName|         LastName|\n+-------------+-----------------+\n|         raja|           pushpa|\n|        priya|           pushpa|\n|      Karthik|             Subu|\n|        James|Smith            |\n|       Martin|            Jones|\n|          Sam| Anderson        |\n|        Maria|          Patrick|\n|        Robet|            Bevon|\n|        Maria|         Anderson|\n+-------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#options3: using dataframe method 2\n",
    "# this method the colmn names are not case sensitive\n",
    "df1 = df.select(df[\"FirstName\"],df[\"LastName\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27255364-a47c-4fb8-9dbe-22d1ec6fa928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<command-601518225244921>\", line 3, in <module>\n    df1 = df.select(df.FirstName, df.LastName)\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 48, in wrapper\n    res = func(*args, **kwargs)\n  File \"/databricks/spark/python/pyspark/sql/dataframe.py\", line 2964, in __getattr__\n    raise AttributeError(\nAttributeError: 'DataFrame' object has no attribute 'FirstName'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n    return FormattedTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n    return VerboseTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n    frames.append(self.format_record(r))\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 730, in format_record\n    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n    pieces = self.included_pieces\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n    pos = scope_pieces.index(self.executing_piece)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n    return only(\n  File \"/databricks/python/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n    raise NotOneValueFound('Expected one value, found 0')\nexecuting.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'DataFrame' object has no attribute 'FirstName'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#option2  uinsg the dataframe method\n",
    "# using this method column name should be case sensitive\n",
    "df1 = df.select(df.FirstName, df.LastName)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d8a655d-100b-417d-99f2-9de63acea17b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n|    firstname|         LastName|\n+-------------+-----------------+\n|         raja|           pushpa|\n|        priya|           pushpa|\n|      Karthik|             Subu|\n|        James|Smith            |\n|       Martin|            Jones|\n|          Sam| Anderson        |\n|        Maria|          Patrick|\n|        Robet|            Bevon|\n|        Maria|         Anderson|\n+-------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# select the columns based on column method\n",
    "# column name is not case sensitive in col method\n",
    "from pyspark.sql.functions import *\n",
    "df2 = df.select(col(\"firstname\"),col(\"LastName\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0285f59-2dfa-469f-b6a1-27ee79ae8a11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n|    firstname|         lastname|\n+-------------+-----------------+\n|         raja|           pushpa|\n|        priya|           pushpa|\n|      Karthik|             Subu|\n|        James|Smith            |\n|       Martin|            Jones|\n|          Sam| Anderson        |\n|        Maria|          Patrick|\n|        Robet|            Bevon|\n|        Maria|         Anderson|\n+-------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# select the column names based on whildcard name\n",
    "df1 = df.select(df.colRegex(\"`.*name*`\"))\n",
    "df1.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-07-07 11:11:31",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}