{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd6beb69-9472-49e6-9942-3057b688f0a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+-----+\n|firstname|lastname|country|state|  sal|\n+---------+--------+-------+-----+-----+\n|     raja|  pushpa|    USA|     |30000|\n|    priya|  pushpa|    USA|     |29900|\n|  Karthik|    Subu|    USA|   CA| 6000|\n|    James|   Smith|    USA|   FL|20000|\n|   Martin|   Jones|    USA|   CA| 3000|\n|      Sam|Anderson|     UK|  LND| 8000|\n|    Maria| Patrick|     UK|  MCR| 7000|\n|    Robet|   Bevon|     UK|  LND| 3500|\n|    Maria|Anderson|     UK|  MCR| 3000|\n+---------+--------+-------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "staticlist = [(\"raja\", \"pushpa\", \"USA\",\"\",30000),\n",
    "            (\"priya\", \"pushpa\", \"USA\",\"\",29900),\n",
    "            (\"Karthik\",\"Subu\", \"USA\",\"CA\",6000),\n",
    "            (\"James\", \"Smith\", \"USA\",\"FL\",20000),\n",
    "            (\"Martin\", \"Jones\", \"USA\",\"CA\",3000),\n",
    "            (\"Sam\", \"Anderson\", \"UK\",\"LND\",8000),\n",
    "            (\"Maria\", \"Patrick\", \"UK\",\"MCR\",7000),\n",
    "            (\"Robet\", \"Bevon\", \"UK\",\"LND\",3500),\n",
    "            (\"Maria\", \"Anderson\", \"UK\",\"MCR\",3000)\n",
    "            ]\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\",\"sal\"]            \n",
    "df = spark.createDataFrame( data = staticlist, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afd0bb46-5852-40f4-918b-0f11416ca721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f463af-b94d-43c6-b0c9-22ea58401aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+-----+--------------+-------------+------------+---------------+------------+\n|firstname|lastname|country|state|  sal|     Full_Name|First4letters|Last4letters|3rdpos_4letters|3rddigitrest|\n+---------+--------+-------+-----+-----+--------------+-------------+------------+---------------+------------+\n|     raja|  pushpa|    USA|     |30000|   Raja Pushpa|         Raja|        shpa|           ja P|   ja Pushpa|\n|    priya|  pushpa|    USA|     |29900|  Priya Pushpa|         Priy|        shpa|           iya |  iya Pushpa|\n|  Karthik|    Subu|    USA|   CA| 6000|  Karthik Subu|         Kart|        Subu|           rthi|  rthik Subu|\n|    James|   Smith|    USA|   FL|20000|   James Smith|         Jame|        mith|           mes |   mes Smith|\n|   Martin|   Jones|    USA|   CA| 3000|  Martin Jones|         Mart|        ones|           rtin|  rtin Jones|\n|      Sam|Anderson|     UK|  LND| 8000|  Sam Anderson|         Sam |        rson|           m An|  m Anderson|\n|    Maria| Patrick|     UK|  MCR| 7000| Maria Patrick|         Mari|        rick|           ria | ria Patrick|\n|    Robet|   Bevon|     UK|  LND| 3500|   Robet Bevon|         Robe|        evon|           bet |   bet Bevon|\n|    Maria|Anderson|     UK|  MCR| 3000|Maria Anderson|         Mari|        rson|           ria |ria Anderson|\n+---------+--------+-------+-----+-----+--------------+-------------+------------+---------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"Full_Name\",trim(concat(initcap(\"firstname\"),lit(\" \"), initcap(\"lastname\"))))\n",
    "df4 = df2.withColumn(\"First4letters\",substring(\"Full_Name\",0,4))\\\n",
    "         .withColumn(\"Last4letters\",substring(\"Full_Name\",-4,4))\\\n",
    "         .withColumn(\"3rdpos_4letters\",substring(\"Full_Name\",3,4))\\\n",
    "         .withColumn(\"3rddigitrest\",substring(\"Full_Name\",3,length(\"Full_Name\")))\n",
    "df4.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efc6a238-8a77-416e-8303-ae52c82e524f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+-----+--------------+-----------------+\n|firstname|lastname|country|state|  sal|     Full_Name|            split|\n+---------+--------+-------+-----+-----+--------------+-----------------+\n|     raja|  pushpa|    USA|     |30000|   Raja Pushpa|   [Raja, Pushpa]|\n|    priya|  pushpa|    USA|     |29900|  Priya Pushpa|  [Priya, Pushpa]|\n|  Karthik|    Subu|    USA|   CA| 6000|  Karthik Subu|  [Karthik, Subu]|\n|    James|   Smith|    USA|   FL|20000|   James Smith|   [James, Smith]|\n|   Martin|   Jones|    USA|   CA| 3000|  Martin Jones|  [Martin, Jones]|\n|      Sam|Anderson|     UK|  LND| 8000|  Sam Anderson|  [Sam, Anderson]|\n|    Maria| Patrick|     UK|  MCR| 7000| Maria Patrick| [Maria, Patrick]|\n|    Robet|   Bevon|     UK|  LND| 3500|   Robet Bevon|   [Robet, Bevon]|\n|    Maria|Anderson|     UK|  MCR| 3000|Maria Anderson|[Maria, Anderson]|\n+---------+--------+-------+-----+-----+--------------+-----------------+\n\n+---------+--------+-------+-----+-----+--------------+-----------------+-----------+------------+\n|firstname|lastname|country|state|  sal|     Full_Name|            split|First_Split|Second_Split|\n+---------+--------+-------+-----+-----+--------------+-----------------+-----------+------------+\n|     raja|  pushpa|    USA|     |30000|   Raja Pushpa|   [Raja, Pushpa]|       Raja|      Pushpa|\n|    priya|  pushpa|    USA|     |29900|  Priya Pushpa|  [Priya, Pushpa]|      Priya|      Pushpa|\n|  Karthik|    Subu|    USA|   CA| 6000|  Karthik Subu|  [Karthik, Subu]|    Karthik|        Subu|\n|    James|   Smith|    USA|   FL|20000|   James Smith|   [James, Smith]|      James|       Smith|\n|   Martin|   Jones|    USA|   CA| 3000|  Martin Jones|  [Martin, Jones]|     Martin|       Jones|\n|      Sam|Anderson|     UK|  LND| 8000|  Sam Anderson|  [Sam, Anderson]|        Sam|    Anderson|\n|    Maria| Patrick|     UK|  MCR| 7000| Maria Patrick| [Maria, Patrick]|      Maria|     Patrick|\n|    Robet|   Bevon|     UK|  LND| 3500|   Robet Bevon|   [Robet, Bevon]|      Robet|       Bevon|\n|    Maria|Anderson|     UK|  MCR| 3000|Maria Anderson|[Maria, Anderson]|      Maria|    Anderson|\n+---------+--------+-------+-----+-----+--------------+-----------------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df3 = df2.withColumn(\"split\",split(\"Full_Name\",\" \"))\n",
    "df3.show()\n",
    "df4 = df3.withColumn(\"First_Split\",df3.split[0])\\\n",
    "         .withColumn(\"Second_Split\",df3.split[1])\n",
    "df4.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8051f132-5056-4444-965f-aaa709993499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+-----+--------------+------+\n|firstname|lastname|country|state|  sal|     Full_Name|length|\n+---------+--------+-------+-----+-----+--------------+------+\n|     raja|  pushpa|    USA|     |30000|   Raja Pushpa|    11|\n|    priya|  pushpa|    USA|     |29900|  Priya Pushpa|    12|\n|  Karthik|    Subu|    USA|   CA| 6000|  Karthik Subu|    12|\n|    James|   Smith|    USA|   FL|20000|   James Smith|    11|\n|   Martin|   Jones|    USA|   CA| 3000|  Martin Jones|    12|\n|      Sam|Anderson|     UK|  LND| 8000|  Sam Anderson|    12|\n|    Maria| Patrick|     UK|  MCR| 7000| Maria Patrick|    13|\n|    Robet|   Bevon|     UK|  LND| 3500|   Robet Bevon|    11|\n|    Maria|Anderson|     UK|  MCR| 3000|Maria Anderson|    14|\n+---------+--------+-------+-----+-----+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# get the lenght of string\n",
    "#length(\"Colname\")\n",
    "df5 = df2.select(df2['*'],length(\"Full_Name\").alias(\"length\"))\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ad9dc0e-4815-4d16-9bd3-f4dfdb477b63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- country: string (nullable = true)\n |-- state: string (nullable = true)\n |-- sal: long (nullable = true)\n |-- Full_Name: string (nullable = true)\n |-- length: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df5.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "471e8df8-068b-461b-b6b6-47168a49257a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+-----+------+\n|firstname|lastname|country|state|  sal|isnull|\n+---------+--------+-------+-----+-----+------+\n|     raja|  pushpa|    USA|     |30000| false|\n|    priya|  pushpa|    USA|     |29900| false|\n|  Karthik|    Subu|    USA|   CA| 6000| false|\n|    James|   Smith|    USA|   FL|20000| false|\n|   Martin|   Jones|    USA|   CA| 3000| false|\n|      Sam|Anderson|     UK|  LND| 8000| false|\n|    Maria| Patrick|     UK|  MCR| 7000| false|\n|    Robet|   Bevon|     UK|  LND| 3500| false|\n|    Maria|Anderson|     UK|  MCR| 3000| false|\n+---------+--------+-------+-----+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "#check if the column is null\n",
    "df6 = df.withColumn(\"isnull\",isnull(\"state\"))\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05c7a92d-60b2-4cbb-a6dc-398ca33ca83d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Expression lanfuage : whenever a pyspark function is not found, we can use a SQL statement inside a pyspark statement using the Expresion language\n",
    "#syntax: expr(\"SQL STATEMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe8d2a9-d640-4eef-b438-5f5d4cb77fec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+-----+\n|firstname|lastname|country|state|  sal|\n+---------+--------+-------+-----+-----+\n|     raja|  pushpa|    USA|     |30000|\n|    priya|  pushpa|    USA|     |29900|\n|  Karthik|    Subu|    USA|   CA| 6000|\n|    James|   Smith|    USA|   FL|20000|\n|   Martin|   Jones|    USA|   CA| 3000|\n|      Sam|Anderson|     UK|  LND| 8000|\n|    Maria| Patrick|     UK|  MCR| 7000|\n|    Robet|   Bevon|     UK|  LND| 3500|\n|    Maria|Anderson|     UK|  MCR| 3000|\n+---------+--------+-------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b71ff990-f000-494d-bfc2-b99cfc612923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+-----+-------------+\n|firstname|lastname|country|state|  sal|    N_Country|\n+---------+--------+-------+-----+-----+-------------+\n|     raja|  pushpa|    USA|     |30000|United States|\n|    priya|  pushpa|    USA|     |29900|United States|\n|  Karthik|    Subu|    USA|   CA| 6000|United States|\n|    James|   Smith|    USA|   FL|20000|United States|\n|   Martin|   Jones|    USA|   CA| 3000|United States|\n|      Sam|Anderson|     UK|  LND| 8000|           UK|\n|    Maria| Patrick|     UK|  MCR| 7000|           UK|\n|    Robet|   Bevon|     UK|  LND| 3500|           UK|\n|    Maria|Anderson|     UK|  MCR| 3000|           UK|\n+---------+--------+-------+-----+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Replace USA with United states in new column\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "df2 = df.withColumn(\"N_Country\",expr(\"CASE WHEN country = 'USA' THEN 'United States' ELSE country END\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1084da9-c02c-45aa-adeb-fde1edcec97f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Date Functions\n",
    "\n",
    "Date usecases\n",
    "# ytd,mtd,qtd,wtd,mom,yoy,year,quarter,month,week,previous year,previous month,convert to date\n",
    "# last 12 months end data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fd6329e-633b-4077-93d7-4eacbf14e4d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+\n|s_no| StartDate|   EndDate|\n+----+----------+----------+\n|   1|2025-01-07|12/31/2024|\n|   2|2024-01-01|11/30/2023|\n|   3|2025-02-21|11/30/2023|\n|   4|2025-02-20|12/31/2024|\n|   5|2025-03-23|01/31/2024|\n|   6|2025-01-23|01/31/2025|\n|   7|2022-04-17|03/30/2022|\n|   8|2022-05-27|03/30/2022|\n+----+----------+----------+\n\nroot\n |-- s_no: long (nullable = true)\n |-- StartDate: string (nullable = true)\n |-- EndDate: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "from delta.tables import *\n",
    "\n",
    "data = [(1,'2025-01-07', '12/31/2024'),\n",
    "        (2,'2024-01-01', '11/30/2023'),\n",
    "        (3,'2025-02-21', '11/30/2023'),\n",
    "        (4,'2025-02-20', '12/31/2024'),\n",
    "        (5,'2025-03-23', '01/31/2024'),\n",
    "        (6,'2025-01-23', '01/31/2025'),\n",
    "        (7,'2022-04-17', '03/30/2022'),\n",
    "        (8,'2022-05-27', '03/30/2022')]\n",
    "        \n",
    "col = ['s_no','StartDate','EndDate'] \n",
    "df = spark.createDataFrame(data = data, schema = col)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b469e2-2291-4a45-ae24-6b710572dee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+\n|s_no| StartDate|   EndDate|\n+----+----------+----------+\n|   1|2025-01-07|2024-12-31|\n|   2|2024-01-01|2023-11-30|\n|   3|2025-02-21|2023-11-30|\n|   4|2025-02-20|2024-12-31|\n|   5|2025-03-23|2024-01-31|\n|   6|2025-01-23|2025-01-31|\n|   7|2022-04-17|2022-03-30|\n|   8|2022-05-27|2022-03-30|\n+----+----------+----------+\n\nroot\n |-- s_no: long (nullable = true)\n |-- StartDate: date (nullable = true)\n |-- EndDate: date (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#cast a string data type to date type\n",
    "#use to_date function to convert the string into a date format\n",
    "#syntax: to_date(\"Datecol\",\"dateformat\")\n",
    "df_cast = df.withColumn(\"StartDate\",df.StartDate.cast(DateType()))\\\n",
    "            .withColumn(\"EndDate\",to_date(\"EndDate\",\"MM/dd/yyyy\"))\n",
    "df_cast.show()\n",
    "df_cast.printSchema()            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab2cfd35-a56e-44ea-a23f-132fa732c8d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----+-------+-----+---+---------+----------+---------+\n|s_no| StartDate|   EndDate|year|quarter|month|day|dayofweek|weekofyear|dayofyear|\n+----+----------+----------+----+-------+-----+---+---------+----------+---------+\n|   1|2025-01-07|12/31/2024|2025|      1|    1|  7|        3|         2|        7|\n|   2|2024-01-01|11/30/2023|2024|      1|    1|  1|        2|         1|        1|\n|   3|2025-02-21|11/30/2023|2025|      1|    2| 21|        6|         8|       52|\n|   4|2025-02-20|12/31/2024|2025|      1|    2| 20|        5|         8|       51|\n|   5|2025-03-23|01/31/2024|2025|      1|    3| 23|        1|        12|       82|\n|   6|2025-01-23|01/31/2025|2025|      1|    1| 23|        5|         4|       23|\n|   7|2022-04-17|03/30/2022|2022|      2|    4| 17|        1|        15|      107|\n|   8|2022-05-27|03/30/2022|2022|      2|    5| 27|        6|        21|      147|\n+----+----------+----------+----+-------+-----+---+---------+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#get the year,month,quarter,day etc..\n",
    "\n",
    "df_dates = df.withColumn(\"year\",year(\"StartDate\"))\\\n",
    "             .withColumn(\"quarter\",quarter(\"StartDate\"))\\\n",
    "             .withColumn(\"month\",month(\"StartDate\"))\\\n",
    "             .withColumn(\"day\",dayofmonth(\"StartDate\"))\\\n",
    "             .withColumn(\"dayofweek\",dayofweek(\"StartDate\"))\\\n",
    "             .withColumn(\"weekofyear\",weekofyear(\"StartDate\"))\\\n",
    "             .withColumn(\"dayofyear\",dayofyear(\"StartDate\"))   \n",
    "             \n",
    "df_dates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c0f9f1-aede-4ccd-ba93-bb6a04657683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+------+---------+-------+----+-------+-----+---+---------+----------+---------+\n|s_no| StartDate|   EndDate|E_year|E_quarter|E_month|year|quarter|month|day|dayofweek|weekofyear|dayofyear|\n+----+----------+----------+------+---------+-------+----+-------+-----+---+---------+----------+---------+\n|   1|2025-01-07|2024-12-31|  2024|        4|     12|2025|      1|    1|  7|        3|         2|        7|\n|   2|2024-01-01|2023-11-30|  2023|        4|     11|2024|      1|    1|  1|        2|         1|        1|\n|   3|2025-02-21|2023-11-30|  2023|        4|     11|2025|      1|    2| 21|        6|         8|       52|\n|   4|2025-02-20|2024-12-31|  2024|        4|     12|2025|      1|    2| 20|        5|         8|       51|\n|   5|2025-03-23|2024-01-31|  2024|        1|      1|2025|      1|    3| 23|        1|        12|       82|\n|   6|2025-01-23|2025-01-31|  2025|        1|      1|2025|      1|    1| 23|        5|         4|       23|\n|   7|2022-04-17|2022-03-30|  2022|        1|      3|2022|      2|    4| 17|        1|        15|      107|\n|   8|2022-05-27|2022-03-30|  2022|        1|      3|2022|      2|    5| 27|        6|        21|      147|\n+----+----------+----------+------+---------+-------+----+-------+-----+---+---------+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_dates = df_cast.withColumn(\"E_year\",year(\"EndDate\"))\\\n",
    "                  .withColumn(\"E_quarter\",quarter(\"EndDate\"))\\\n",
    "                  .withColumn(\"E_month\",month(\"EndDate\"))\\\n",
    "                  .withColumn(\"year\",year(\"StartDate\"))\\\n",
    "                   .withColumn(\"quarter\",quarter(\"StartDate\"))\\\n",
    "                   .withColumn(\"month\",month(\"StartDate\"))\\\n",
    "                   .withColumn(\"day\",dayofmonth(\"StartDate\"))\\\n",
    "                   .withColumn(\"dayofweek\",dayofweek(\"StartDate\"))\\\n",
    "                   .withColumn(\"weekofyear\",weekofyear(\"StartDate\"))\\\n",
    "                   .withColumn(\"dayofyear\",dayofyear(\"StartDate\"))     \n",
    "\n",
    "df_dates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af7eade0-845d-499f-9edd-83b89ba7091f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------+----------+\n|s_no| StartDate|   EndDate| Add2_D_SD| Add2_D_ED|\n+----+----------+----------+----------+----------+\n|   1|2025-01-07|2024-12-31|2025-01-09|2025-01-02|\n|   2|2024-01-01|2023-11-30|2024-01-03|2023-12-02|\n|   3|2025-02-21|2023-11-30|2025-02-23|2023-12-02|\n|   4|2025-02-20|2024-12-31|2025-02-22|2025-01-02|\n|   5|2025-03-23|2024-01-31|2025-03-25|2024-02-02|\n|   6|2025-01-23|2025-01-31|2025-01-25|2025-02-02|\n|   7|2022-04-17|2022-03-30|2022-04-19|2022-04-01|\n|   8|2022-05-27|2022-03-30|2022-05-29|2022-04-01|\n+----+----------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Add date to Existing dates\n",
    "df_add1 = df_cast.withColumn(\"Add2_D_SD\",date_add(\"StartDate\",2))\\\n",
    "                  .withColumn(\"Add2_D_ED\",date_add(\"EndDate\",2))\n",
    "df_add1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f243079d-2682-46b2-b76c-4192f67f2d22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------+----------+----------+----------+\n|s_no| StartDate|   EndDate| Add2_D_SD| Add2_D_ED|   Rm_D_SD|   Rm_D_ED|\n+----+----------+----------+----------+----------+----------+----------+\n|   1|2025-01-07|2024-12-31|2025-01-09|2025-01-02|2025-01-05|2024-12-29|\n|   2|2024-01-01|2023-11-30|2024-01-03|2023-12-02|2023-12-30|2023-11-28|\n|   3|2025-02-21|2023-11-30|2025-02-23|2023-12-02|2025-02-19|2023-11-28|\n|   4|2025-02-20|2024-12-31|2025-02-22|2025-01-02|2025-02-18|2024-12-29|\n|   5|2025-03-23|2024-01-31|2025-03-25|2024-02-02|2025-03-21|2024-01-29|\n|   6|2025-01-23|2025-01-31|2025-01-25|2025-02-02|2025-01-21|2025-01-29|\n|   7|2022-04-17|2022-03-30|2022-04-19|2022-04-01|2022-04-15|2022-03-28|\n|   8|2022-05-27|2022-03-30|2022-05-29|2022-04-01|2022-05-25|2022-03-28|\n+----+----------+----------+----------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Remove Days from Existing date\n",
    "df_rmv = df_add1.withColumn(\"Rm_D_SD\",date_sub(\"StartDate\",2))\\\n",
    "                .withColumn(\"Rm_D_ED\",date_sub(\"EndDate\",2))\n",
    "df_rmv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15c38846-df70-4c98-b9e2-7faa56bf1add",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------+----------+\n|s_no| StartDate|   EndDate| Add2_M_SD| Add2_M_ED|\n+----+----------+----------+----------+----------+\n|   1|2025-01-07|2024-12-31|2025-03-07|2025-02-28|\n|   2|2024-01-01|2023-11-30|2024-03-01|2024-01-30|\n|   3|2025-02-21|2023-11-30|2025-04-21|2024-01-30|\n|   4|2025-02-20|2024-12-31|2025-04-20|2025-02-28|\n|   5|2025-03-23|2024-01-31|2025-05-23|2024-03-31|\n|   6|2025-01-23|2025-01-31|2025-03-23|2025-03-31|\n|   7|2022-04-17|2022-03-30|2022-06-17|2022-05-30|\n|   8|2022-05-27|2022-03-30|2022-07-27|2022-05-30|\n+----+----------+----------+----------+----------+\n\n+----+----------+----------+----------+----------+\n|s_no| StartDate|   EndDate|   Rm_M_SD|   Rm_M_ED|\n+----+----------+----------+----------+----------+\n|   1|2025-01-07|2024-12-31|2024-11-07|2024-10-31|\n|   2|2024-01-01|2023-11-30|2023-11-01|2023-09-30|\n|   3|2025-02-21|2023-11-30|2024-12-21|2023-09-30|\n|   4|2025-02-20|2024-12-31|2024-12-20|2024-10-31|\n|   5|2025-03-23|2024-01-31|2025-01-23|2023-11-30|\n|   6|2025-01-23|2025-01-31|2024-11-23|2024-11-30|\n|   7|2022-04-17|2022-03-30|2022-02-17|2022-01-30|\n|   8|2022-05-27|2022-03-30|2022-03-27|2022-01-30|\n+----+----------+----------+----------+----------+\n\n+----+----------+----------+----------+----------+----------+----------+\n|s_no| StartDate|   EndDate| Add2_M_SD| Add2_M_ED|   Rm_M_SD|   Rm_M_ED|\n+----+----------+----------+----------+----------+----------+----------+\n|   1|2025-01-07|2024-12-31|2025-03-07|2025-02-28|2024-11-07|2024-10-31|\n|   2|2024-01-01|2023-11-30|2024-03-01|2024-01-30|2023-11-01|2023-09-30|\n|   3|2025-02-21|2023-11-30|2025-04-21|2024-01-30|2024-12-21|2023-09-30|\n|   4|2025-02-20|2024-12-31|2025-04-20|2025-02-28|2024-12-20|2024-10-31|\n|   5|2025-03-23|2024-01-31|2025-05-23|2024-03-31|2025-01-23|2023-11-30|\n|   6|2025-01-23|2025-01-31|2025-03-23|2025-03-31|2024-11-23|2024-11-30|\n|   7|2022-04-17|2022-03-30|2022-06-17|2022-05-30|2022-02-17|2022-01-30|\n|   8|2022-05-27|2022-03-30|2022-07-27|2022-05-30|2022-03-27|2022-01-30|\n+----+----------+----------+----------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#add, remove months to existing date\n",
    "df_add2 = df_cast.withColumn(\"Add2_M_SD\",add_months(\"StartDate\",2))\\\n",
    "            .withColumn(\"Add2_M_ED\",add_months(\"EndDate\",2))\n",
    "df_add2.show()\n",
    "df_rmv2 = df_cast.withColumn(\"Rm_M_SD\",add_months(\"StartDate\",-2))\\\n",
    "            .withColumn(\"Rm_M_ED\",add_months(\"EndDate\",-2))\n",
    "df_rmv2.show()\n",
    "df_rmv3 = df_add2.withColumn(\"Rm_M_SD\",add_months(\"StartDate\",-2))\\\n",
    "            .withColumn(\"Rm_M_ED\",add_months(\"EndDate\",-2))\n",
    "df_rmv3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee7828eb-069b-4685-9a04-6979a49b513f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|s_no| StartDate|   EndDate|          Add2_Y_SD|          Add2_Y_ED|          Add2_Q_SD|          Add2_Q_ED|          Add2_W_SD|          Add2_W_ED|\n+----+----------+----------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n|   1|2025-01-07|2024-12-31|2027-01-07 00:00:00|2026-12-31 00:00:00|2025-07-07 00:00:00|2025-06-30 00:00:00|2025-01-21 00:00:00|2025-01-14 00:00:00|\n|   2|2024-01-01|2023-11-30|2026-01-01 00:00:00|2025-11-30 00:00:00|2024-07-01 00:00:00|2024-05-30 00:00:00|2024-01-15 00:00:00|2023-12-14 00:00:00|\n|   3|2025-02-21|2023-11-30|2027-02-21 00:00:00|2025-11-30 00:00:00|2025-08-21 00:00:00|2024-05-30 00:00:00|2025-03-07 00:00:00|2023-12-14 00:00:00|\n|   4|2025-02-20|2024-12-31|2027-02-20 00:00:00|2026-12-31 00:00:00|2025-08-20 00:00:00|2025-06-30 00:00:00|2025-03-06 00:00:00|2025-01-14 00:00:00|\n|   5|2025-03-23|2024-01-31|2027-03-23 00:00:00|2026-01-31 00:00:00|2025-09-23 00:00:00|2024-07-31 00:00:00|2025-04-06 00:00:00|2024-02-14 00:00:00|\n|   6|2025-01-23|2025-01-31|2027-01-23 00:00:00|2027-01-31 00:00:00|2025-07-23 00:00:00|2025-07-31 00:00:00|2025-02-06 00:00:00|2025-02-14 00:00:00|\n|   7|2022-04-17|2022-03-30|2024-04-17 00:00:00|2024-03-30 00:00:00|2022-10-17 00:00:00|2022-09-30 00:00:00|2022-05-01 00:00:00|2022-04-13 00:00:00|\n|   8|2022-05-27|2022-03-30|2024-05-27 00:00:00|2024-03-30 00:00:00|2022-11-27 00:00:00|2022-09-30 00:00:00|2022-06-10 00:00:00|2022-04-13 00:00:00|\n+----+----------+----------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Alternate otion to use date add for yer,quarter,week\n",
    "df_add3 = df_cast.withColumn(\"Add2_Y_SD\",expr(\"DATEADD(YEAR,2,StartDate)\"))\\\n",
    "                 .withColumn(\"Add2_Y_ED\",expr(\"DATEADD(YEAR,2,EndDate)\"))\\\n",
    "                 .withColumn(\"Add2_Q_SD\",expr(\"DATEADD(QUARTER,2,StartDate)\"))\\\n",
    "                 .withColumn(\"Add2_Q_ED\",expr(\"DATEADD(QUARTER,2,EndDate)\"))\\\n",
    "                 .withColumn(\"Add2_W_SD\",expr(\"DATEADD(WEEK,2,StartDate)\"))\\\n",
    "                 .withColumn(\"Add2_W_ED\",expr(\"DATEADD(WEEK,2,EndDate)\"))      \n",
    "df_add3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d7e702d-d9c4-492f-ac59-bf971084f712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------+\n|s_no| StartDate|   EndDate|  Last_Day|\n+----+----------+----------+----------+\n|   1|2025-01-07|12/31/2024|2025-01-31|\n|   2|2024-01-01|11/30/2023|2024-01-31|\n|   3|2025-02-21|11/30/2023|2025-02-28|\n|   4|2025-02-20|12/31/2024|2025-02-28|\n|   5|2025-03-23|01/31/2024|2025-03-31|\n|   6|2025-01-23|01/31/2025|2025-01-31|\n|   7|2022-04-17|03/30/2022|2022-04-30|\n|   8|2022-05-27|03/30/2022|2022-05-31|\n+----+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Last day of the month\n",
    "\n",
    "df_lst = df.withColumn(\"Last_Day\",last_day(\"StartDate\"))\n",
    "df_lst.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "addd84ec-ee79-4750-b547-843e2155cadf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+------------+\n|s_no| StartDate|   EndDate|current_date|\n+----+----------+----------+------------+\n|   1|2025-01-07|12/31/2024|  2025-07-10|\n|   2|2024-01-01|11/30/2023|  2025-07-10|\n|   3|2025-02-21|11/30/2023|  2025-07-10|\n|   4|2025-02-20|12/31/2024|  2025-07-10|\n|   5|2025-03-23|01/31/2024|  2025-07-10|\n|   6|2025-01-23|01/31/2025|  2025-07-10|\n|   7|2022-04-17|03/30/2022|  2025-07-10|\n|   8|2022-05-27|03/30/2022|  2025-07-10|\n+----+----------+----------+------------+\n\n+----+----------+----------+--------------------+\n|s_no| StartDate|   EndDate|   current_timestamp|\n+----+----------+----------+--------------------+\n|   1|2025-01-07|12/31/2024|2025-07-10 06:53:...|\n|   2|2024-01-01|11/30/2023|2025-07-10 06:53:...|\n|   3|2025-02-21|11/30/2023|2025-07-10 06:53:...|\n|   4|2025-02-20|12/31/2024|2025-07-10 06:53:...|\n|   5|2025-03-23|01/31/2024|2025-07-10 06:53:...|\n|   6|2025-01-23|01/31/2025|2025-07-10 06:53:...|\n|   7|2022-04-17|03/30/2022|2025-07-10 06:53:...|\n|   8|2022-05-27|03/30/2022|2025-07-10 06:53:...|\n+----+----------+----------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#current dat will give the output as date only\n",
    "\n",
    "df_cur = df.withColumn(\"current_date\",current_date())\n",
    "df_cur.show()\n",
    "\n",
    "#current timestamp will give the output as timestamp\n",
    "\n",
    "df_cur = df.withColumn(\"current_timestamp\",current_timestamp())\n",
    "df_cur.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39061833-5dfa-422f-a9a7-c8671fb922b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+-------------------+-------------------+-------------------+-------------------+\n|s_no| StartDate|   EndDate|           ST_of_yr|        ST_of_month|      ST_of_quarter|         ST_of_week|\n+----+----------+----------+-------------------+-------------------+-------------------+-------------------+\n|   1|2025-01-07|12/31/2024|2025-01-01 00:00:00|2025-01-01 00:00:00|2025-01-01 00:00:00|2025-01-06 00:00:00|\n|   2|2024-01-01|11/30/2023|2024-01-01 00:00:00|2024-01-01 00:00:00|2024-01-01 00:00:00|2024-01-01 00:00:00|\n|   3|2025-02-21|11/30/2023|2025-01-01 00:00:00|2025-02-01 00:00:00|2025-01-01 00:00:00|2025-02-17 00:00:00|\n|   4|2025-02-20|12/31/2024|2025-01-01 00:00:00|2025-02-01 00:00:00|2025-01-01 00:00:00|2025-02-17 00:00:00|\n|   5|2025-03-23|01/31/2024|2025-01-01 00:00:00|2025-03-01 00:00:00|2025-01-01 00:00:00|2025-03-17 00:00:00|\n|   6|2025-01-23|01/31/2025|2025-01-01 00:00:00|2025-01-01 00:00:00|2025-01-01 00:00:00|2025-01-20 00:00:00|\n|   7|2022-04-17|03/30/2022|2022-01-01 00:00:00|2022-04-01 00:00:00|2022-04-01 00:00:00|2022-04-11 00:00:00|\n|   8|2022-05-27|03/30/2022|2022-01-01 00:00:00|2022-05-01 00:00:00|2022-04-01 00:00:00|2022-05-23 00:00:00|\n+----+----------+----------+-------------------+-------------------+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# date_trunc will take you the start of the year, month,week\n",
    "#yntax date_trunc('interval','datecol')\n",
    "\n",
    "df_trunc = df.withColumn(\"ST_of_yr\",date_trunc(\"year\",\"StartDate\"))\\\n",
    "             .withColumn(\"ST_of_month\",date_trunc(\"month\",\"StartDate\"))\\\n",
    "             .withColumn(\"ST_of_quarter\",date_trunc(\"quarter\",\"StartDate\"))\\\n",
    "             .withColumn(\"ST_of_week\",date_trunc(\"week\",\"StartDate\"))\n",
    "df_trunc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dca937af-1473-431e-a781-ce8ea1cb9581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+------+\n|s_no| StartDate|   EndDate|   fmt|\n+----+----------+----------+------+\n|   1|2025-01-07|12/31/2024|Jan-07|\n|   2|2024-01-01|11/30/2023|Jan-01|\n|   3|2025-02-21|11/30/2023|Feb-21|\n|   4|2025-02-20|12/31/2024|Feb-20|\n|   5|2025-03-23|01/31/2024|Mar-23|\n|   6|2025-01-23|01/31/2025|Jan-23|\n|   7|2022-04-17|03/30/2022|Apr-17|\n|   8|2022-05-27|03/30/2022|May-27|\n+----+----------+----------+------+\n\n+----+----------+----------+-----------+\n|s_no| StartDate|   EndDate|        fmt|\n+----+----------+----------+-----------+\n|   1|2025-01-07|12/31/2024| January-07|\n|   2|2024-01-01|11/30/2023| January-01|\n|   3|2025-02-21|11/30/2023|February-21|\n|   4|2025-02-20|12/31/2024|February-20|\n|   5|2025-03-23|01/31/2024|   March-23|\n|   6|2025-01-23|01/31/2025| January-23|\n|   7|2022-04-17|03/30/2022|   April-17|\n|   8|2022-05-27|03/30/2022|     May-27|\n+----+----------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# get the date format in \"JUL-10\"\n",
    "\n",
    "df_fmt = df.withColumn(\"fmt\",date_format(\"StartDate\",\"MMM-dd\"))\n",
    "df_fmt.show()\n",
    "\n",
    "# get the date format in \"JULy-10\"\n",
    "\n",
    "df_fmt1 = df.withColumn(\"fmt\",date_format(\"StartDate\",\"MMMM-dd\"))\n",
    "df_fmt1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac7484d2-602d-41d2-a6f0-8030312462e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+--------------------+\n|s_no| StartDate|   EndDate|          current_TS|\n+----+----------+----------+--------------------+\n|   1|2025-01-07|12/31/2024|2025-07-10 07:03:...|\n|   2|2024-01-01|11/30/2023|2025-07-10 07:03:...|\n|   3|2025-02-21|11/30/2023|2025-07-10 07:03:...|\n|   4|2025-02-20|12/31/2024|2025-07-10 07:03:...|\n|   5|2025-03-23|01/31/2024|2025-07-10 07:03:...|\n|   6|2025-01-23|01/31/2025|2025-07-10 07:03:...|\n|   7|2022-04-17|03/30/2022|2025-07-10 07:03:...|\n|   8|2022-05-27|03/30/2022|2025-07-10 07:03:...|\n+----+----------+----------+--------------------+\n\n+----+----------+----------+--------------------+----+------+------+\n|s_no| StartDate|   EndDate|          current_TS|hour|minute|second|\n+----+----------+----------+--------------------+----+------+------+\n|   1|2025-01-07|12/31/2024|2025-07-10 07:03:...|   7|     3|     1|\n|   2|2024-01-01|11/30/2023|2025-07-10 07:03:...|   7|     3|     1|\n|   3|2025-02-21|11/30/2023|2025-07-10 07:03:...|   7|     3|     1|\n|   4|2025-02-20|12/31/2024|2025-07-10 07:03:...|   7|     3|     1|\n|   5|2025-03-23|01/31/2024|2025-07-10 07:03:...|   7|     3|     1|\n|   6|2025-01-23|01/31/2025|2025-07-10 07:03:...|   7|     3|     1|\n|   7|2022-04-17|03/30/2022|2025-07-10 07:03:...|   7|     3|     1|\n|   8|2022-05-27|03/30/2022|2025-07-10 07:03:...|   7|     3|     1|\n+----+----------+----------+--------------------+----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# get the hour, minutes, seconds from  a date\n",
    "df2 = df.withColumn(\"current_TS\",current_timestamp())\n",
    "df2.show()\n",
    "df3 = df2.withColumn(\"hour\",hour(\"current_TS\"))\\\n",
    "         .withColumn(\"minute\",minute(\"current_TS\"))\\\n",
    "         .withColumn(\"second\",second(\"current_TS\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c438a9ce-41b1-4b7f-af90-d6b83765a8cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+--------------------+\n|s_no| StartDate|   EndDate|          current_TS|\n+----+----------+----------+--------------------+\n|   1|2025-01-07|12/31/2024|2025-07-10 07:06:...|\n|   2|2024-01-01|11/30/2023|2025-07-10 07:06:...|\n|   3|2025-02-21|11/30/2023|2025-07-10 07:06:...|\n|   4|2025-02-20|12/31/2024|2025-07-10 07:06:...|\n|   5|2025-03-23|01/31/2024|2025-07-10 07:06:...|\n|   6|2025-01-23|01/31/2025|2025-07-10 07:06:...|\n|   7|2022-04-17|03/30/2022|2025-07-10 07:06:...|\n|   8|2022-05-27|03/30/2022|2025-07-10 07:06:...|\n+----+----------+----------+--------------------+\n\n+----+----------+----------+--------------------+---------+\n|s_no| StartDate|   EndDate|          current_TS|diff_days|\n+----+----------+----------+--------------------+---------+\n|   1|2025-01-07|12/31/2024|2025-07-10 07:06:...|      184|\n|   2|2024-01-01|11/30/2023|2025-07-10 07:06:...|      556|\n|   3|2025-02-21|11/30/2023|2025-07-10 07:06:...|      139|\n|   4|2025-02-20|12/31/2024|2025-07-10 07:06:...|      140|\n|   5|2025-03-23|01/31/2024|2025-07-10 07:06:...|      109|\n|   6|2025-01-23|01/31/2025|2025-07-10 07:06:...|      168|\n|   7|2022-04-17|03/30/2022|2025-07-10 07:06:...|     1180|\n|   8|2022-05-27|03/30/2022|2025-07-10 07:06:...|     1140|\n+----+----------+----------+--------------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#datefind function will return the difference in the form of no of days\n",
    "\n",
    "df2 = df.withColumn(\"current_TS\",current_timestamp())\n",
    "df2.show()\n",
    "df3 = df2.withColumn(\"diff_days\",datediff(\"current_TS\",\"StartDate\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d11bc87-da50-4f22-b2a3-474e693e3101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+-------------------+--------------------+\n|s_no| StartDate|   EndDate|     Start_of_today|          current_TS|\n+----+----------+----------+-------------------+--------------------+\n|   1|2025-01-07|12/31/2024|2025-07-10 00:00:00|2025-07-10 07:16:...|\n|   2|2024-01-01|11/30/2023|2025-07-10 00:00:00|2025-07-10 07:16:...|\n|   3|2025-02-21|11/30/2023|2025-07-10 00:00:00|2025-07-10 07:16:...|\n|   4|2025-02-20|12/31/2024|2025-07-10 00:00:00|2025-07-10 07:16:...|\n|   5|2025-03-23|01/31/2024|2025-07-10 00:00:00|2025-07-10 07:16:...|\n|   6|2025-01-23|01/31/2025|2025-07-10 00:00:00|2025-07-10 07:16:...|\n|   7|2022-04-17|03/30/2022|2025-07-10 00:00:00|2025-07-10 07:16:...|\n|   8|2022-05-27|03/30/2022|2025-07-10 00:00:00|2025-07-10 07:16:...|\n+----+----------+----------+-------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#get the difference in time b/w start date and todays date current time\n",
    "\n",
    "df2 = df.withColumn(\"Start_of_today\",date_trunc(\"day\",current_timestamp()))\\\n",
    "        .withColumn(\"current_TS\",current_timestamp())\n",
    "df2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffbe0ddf-ca83-4fe2-9d24-874b3bdf67ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "get the length count of firstsplit and second split"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-07-10 11:10:25",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}