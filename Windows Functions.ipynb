{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "898a37a7-0ade-4847-875f-672b1fddde35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "rown_number(), rank, dense_rank, lead, lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e77ac92-b83b-4f15-a4b0-6b78c44eecec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sampleData = [\n",
    "('Tom Prescott', 'Furniture', 'Chairs', 4000),\n",
    "('Quincy Jones', 'Furniture', 'Bookcases', 4000),\n",
    "('Joseph Holt', 'Furniture', 'Tables', 5000),\n",
    "('Alejandro Grove', 'Furniture', 'Furnishings', 8000),\n",
    "('Adrian Barton', 'Office Supplies', 'Binders', 3000),\n",
    "('Ken Lonsdale', 'Office Supplies', 'Supplies', 9000),\n",
    "('Greg Guthrie', 'Office Supplies', 'Fasteners', 3000),\n",
    "('Yoseph Carroll', 'Office Supplies', 'Storage', 3000),\n",
    "('Sean Miller', 'Technology', 'Machines', 22000),\n",
    "('Tamara Chand', 'Technology', 'Copiers', 18000),\n",
    "('John Murray', 'Technology', 'Phones', 5000),\n",
    "('Kelly Collister', 'Technology', 'Accessories', 3000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0cdd17e-3ad0-412e-ab69-32c1f99ed080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# column names for dataframe\n",
    "columns = [\"Customer_Name\",\"Category\",\"Subcategory\",\"Sales\"]\n",
    "# create dataframe\n",
    "df = spark.createDataFrame(data = sampleData,schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed8340dc-3d17-4b7c-8bd4-a939d4e8990a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+-----------+-----+\n|  Customer_Name|       Category|Subcategory|Sales|\n+---------------+---------------+-----------+-----+\n|   Tom Prescott|      Furniture|     Chairs| 4000|\n|   Quincy Jones|      Furniture|  Bookcases| 4000|\n|    Joseph Holt|      Furniture|     Tables| 5000|\n|Alejandro Grove|      Furniture|Furnishings| 8000|\n|  Adrian Barton|Office Supplies|    Binders| 3000|\n|   Ken Lonsdale|Office Supplies|   Supplies| 9000|\n|   Greg Guthrie|Office Supplies|  Fasteners| 3000|\n| Yoseph Carroll|Office Supplies|    Storage| 3000|\n|    Sean Miller|     Technology|   Machines|22000|\n|   Tamara Chand|     Technology|    Copiers|18000|\n|    John Murray|     Technology|     Phones| 5000|\n|Kelly Collister|     Technology|Accessories| 3000|\n+---------------+---------------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aae9de2d-5db0-46a6-9bcb-2c97dd25174d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * # this has to used whenever you are working with windows functions\n",
    "from pyspark.sql.window import *   # this has to used whenever you are trying to use transformation funcrtions\n",
    "from pyspark.sql.types import *    # this has to used whenever you are trying to assign datatypes and strcut  field and struct type\n",
    "from pyspark.sql.column import *   # this has to used whenever you are  working with col in dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17e0124a-48c4-452e-a131-0cb7bcafb3b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* to create a window function in pyspark, there are two steps \n",
    "   * step1 : assign a partition and oreder level, either table or specific column partition levels\n",
    "   * teps2 : write any fuynction based on above partion created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fe08979-9900-4a5e-b1c0-46e579df548b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Example for SQL tabel level : over (order by sales dec)\n",
    "pyspark code for tabel level: Window.orderBy(col(\"Sales\").desc())\n",
    "\n",
    "Example for SQL partition level: over (partition by category order by sales desc)\n",
    "pyspark code for partition level: Window.partitionBy(\"Category\").orderBy(col(\"Sales\").desc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0201436-866c-453a-865c-faa242e199b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+-----------+-----+----------+\n|  Customer_Name|       Category|Subcategory|Sales|row_number|\n+---------------+---------------+-----------+-----+----------+\n|    Sean Miller|     Technology|   Machines|22000|         1|\n|   Tamara Chand|     Technology|    Copiers|18000|         2|\n|   Ken Lonsdale|Office Supplies|   Supplies| 9000|         3|\n|Alejandro Grove|      Furniture|Furnishings| 8000|         4|\n|    Joseph Holt|      Furniture|     Tables| 5000|         5|\n|    John Murray|     Technology|     Phones| 5000|         6|\n|   Tom Prescott|      Furniture|     Chairs| 4000|         7|\n|   Quincy Jones|      Furniture|  Bookcases| 4000|         8|\n|  Adrian Barton|Office Supplies|    Binders| 3000|         9|\n|   Greg Guthrie|Office Supplies|  Fasteners| 3000|        10|\n| Yoseph Carroll|Office Supplies|    Storage| 3000|        11|\n|Kelly Collister|     Technology|Accessories| 3000|        12|\n+---------------+---------------+-----------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#create a row_number() for above table based on sales descending order\n",
    "# step:1: create a windows partition at table level\n",
    "table_partition = Window.orderBy(col(\"Sales\").desc())\n",
    "\n",
    "#step2: create a row_number function\n",
    "\n",
    "df2 = df.withColumn(\"row_number\",row_number().over(table_partition))\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8174ace6-64ae-4fe1-82e6-03c0c8955791",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+-----------+-----+----------+----+----------+\n|  Customer_Name|       Category|Subcategory|Sales|row_number|Rank|Dense_rank|\n+---------------+---------------+-----------+-----+----------+----+----------+\n|Alejandro Grove|      Furniture|Furnishings| 8000|         1|   1|         1|\n|    Joseph Holt|      Furniture|     Tables| 5000|         2|   2|         2|\n|   Tom Prescott|      Furniture|     Chairs| 4000|         3|   3|         3|\n|   Quincy Jones|      Furniture|  Bookcases| 4000|         4|   3|         3|\n|   Ken Lonsdale|Office Supplies|   Supplies| 9000|         1|   1|         1|\n|  Adrian Barton|Office Supplies|    Binders| 3000|         2|   2|         2|\n|   Greg Guthrie|Office Supplies|  Fasteners| 3000|         3|   2|         2|\n| Yoseph Carroll|Office Supplies|    Storage| 3000|         4|   2|         2|\n|    Sean Miller|     Technology|   Machines|22000|         1|   1|         1|\n|   Tamara Chand|     Technology|    Copiers|18000|         2|   2|         2|\n|    John Murray|     Technology|     Phones| 5000|         3|   3|         3|\n|Kelly Collister|     Technology|Accessories| 3000|         4|   4|         4|\n+---------------+---------------+-----------+-----+----------+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# craete a row_number() for above table based on category partiton and sales descending\n",
    "\n",
    "# Step1: create a windows partition at at level\n",
    "window_cat = Window.partitionBy(\"Category\").orderBy(col(\"Sales\").desc())\n",
    "\n",
    "#step2:  Create a row_number function\n",
    "df = df.withColumn(\"row_number\",row_number().over(window_cat))\\\n",
    ".withColumn(\"Rank\",rank().over(window_cat))\\\n",
    "    .withColumn(\"Dense_rank\",dense_rank().over(window_cat))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08de98b3-ab5d-44b7-94c0-c0f0c9ef235c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "*LED : Get the next row value into current row\n",
    "** syntax: lead(\"COLNAME\",<NO_OF_NextColumn>).over(<PARTITION LEVEL>)\n",
    "* LAG : get egh last row value into currrent row\n",
    "** syntax: lag(\"COLNAME\",<no_of_lastcolumn>).over(<PARTITION LEVEL>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b50c524-d2a1-4f10-abc8-daf7841fa5af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+-----------+-----+----------+----+----------+-----+-----+\n|  Customer_Name|       Category|Subcategory|Sales|row_number|Rank|Dense_rank| lag1| lag2|\n+---------------+---------------+-----------+-----+----------+----+----------+-----+-----+\n|    Sean Miller|     Technology|   Machines|22000|         1|   1|         1| NULL| NULL|\n|   Tamara Chand|     Technology|    Copiers|18000|         2|   2|         2|22000| NULL|\n|   Ken Lonsdale|Office Supplies|   Supplies| 9000|         1|   1|         1|18000|22000|\n|Alejandro Grove|      Furniture|Furnishings| 8000|         1|   1|         1| 9000|18000|\n|    Joseph Holt|      Furniture|     Tables| 5000|         2|   2|         2| 8000| 9000|\n|    John Murray|     Technology|     Phones| 5000|         3|   3|         3| 5000| 8000|\n|   Tom Prescott|      Furniture|     Chairs| 4000|         3|   3|         3| 5000| 5000|\n|   Quincy Jones|      Furniture|  Bookcases| 4000|         4|   3|         3| 4000| 5000|\n|  Adrian Barton|Office Supplies|    Binders| 3000|         2|   2|         2| 4000| 4000|\n|   Greg Guthrie|Office Supplies|  Fasteners| 3000|         3|   2|         2| 3000| 4000|\n| Yoseph Carroll|Office Supplies|    Storage| 3000|         4|   2|         2| 3000| 3000|\n|Kelly Collister|     Technology|Accessories| 3000|         4|   4|         4| 3000| 3000|\n+---------------+---------------+-----------+-----+----------+----+----------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# get teh previous row sales into current rtow\n",
    "#step1: Create a window partiotn at table level\n",
    "table_partition = Window.orderBy(col(\"Sales\").desc())\n",
    "#step2: Create a lag function\n",
    "d2f = df.withColumn(\"lag1\",lag(\"Sales\",1).over(table_partition))\\\n",
    "    .withColumn(\"lag2\",lag(\"Sales\",2).over(table_partition))\n",
    "d2f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a0f5d90-9162-4467-be68-90df3d8fc1a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+-----------+-----+----------+----+----------+-----+-----+\n|  Customer_Name|       Category|Subcategory|Sales|row_number|Rank|Dense_rank|lead1|lead2|\n+---------------+---------------+-----------+-----+----------+----+----------+-----+-----+\n|    Sean Miller|     Technology|   Machines|22000|         1|   1|         1|18000| 9000|\n|   Tamara Chand|     Technology|    Copiers|18000|         2|   2|         2| 9000| 8000|\n|   Ken Lonsdale|Office Supplies|   Supplies| 9000|         1|   1|         1| 8000| 5000|\n|Alejandro Grove|      Furniture|Furnishings| 8000|         1|   1|         1| 5000| 5000|\n|    Joseph Holt|      Furniture|     Tables| 5000|         2|   2|         2| 5000| 4000|\n|    John Murray|     Technology|     Phones| 5000|         3|   3|         3| 4000| 4000|\n|   Tom Prescott|      Furniture|     Chairs| 4000|         3|   3|         3| 4000| 3000|\n|   Quincy Jones|      Furniture|  Bookcases| 4000|         4|   3|         3| 3000| 3000|\n|  Adrian Barton|Office Supplies|    Binders| 3000|         2|   2|         2| 3000| 3000|\n|   Greg Guthrie|Office Supplies|  Fasteners| 3000|         3|   2|         2| 3000| 3000|\n| Yoseph Carroll|Office Supplies|    Storage| 3000|         4|   2|         2| 3000| NULL|\n|Kelly Collister|     Technology|Accessories| 3000|         4|   4|         4| NULL| NULL|\n+---------------+---------------+-----------+-----+----------+----+----------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#get the next row sales into current row\n",
    "\n",
    "#step1: creta a window partiotn at table level\n",
    "table_partition = Window.orderBy(col(\"Sales\").desc())\n",
    "#step2: Create a lead function\n",
    "df2 = df.withColumn(\"lead1\",lead(\"Sales\",1).over(table_partition))\\\n",
    "    .withColumn(\"lead2\",lead(\"Sales\",2).over(table_partition))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3af948c-c3c5-40fe-8889-c3e926f57dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+-----------+-----+----------+----+----------+\n|  Customer_Name|       Category|Subcategory|Sales|row_number|rank|Dense_rank|\n+---------------+---------------+-----------+-----+----------+----+----------+\n|Alejandro Grove|      Furniture|Furnishings| 8000|         1|   1|         1|\n|    Joseph Holt|      Furniture|     Tables| 5000|         2|   2|         2|\n|   Ken Lonsdale|Office Supplies|   Supplies| 9000|         1|   1|         1|\n|  Adrian Barton|Office Supplies|    Binders| 3000|         2|   2|         2|\n|   Greg Guthrie|Office Supplies|  Fasteners| 3000|         3|   2|         2|\n| Yoseph Carroll|Office Supplies|    Storage| 3000|         4|   2|         2|\n|    Sean Miller|     Technology|   Machines|22000|         1|   1|         1|\n|   Tamara Chand|     Technology|    Copiers|18000|         2|   2|         2|\n+---------------+---------------+-----------+-----+----------+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#get top 2 sales from each category\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "partion_cat = Window.partitionBy(\"Category\").orderBy(col(\"Sales\").desc())\n",
    "df2 = df.withColumn(\"rank\",rank().over(partion_cat))\n",
    "df2 = df2.filter(\"rank <=2\")\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f3621e-4094-4886-833e-d7867dc2d9d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----+\n| id|   name|      date|value|\n+---+-------+----------+-----+\n|  1|  Alice|2024-01-01|    A|\n|  1|  Alice|2024-01-02|    B|\n|  1|  Alice|2024-01-03|    C|\n|  1|  Alice|2024-01-04|    D|\n|  2|    Bob|2024-02-01|    E|\n|  2|    Bob|2024-02-02|    F|\n|  2|    Bob|2024-02-03|    G|\n|  2|    Bob|2024-02-04|    H|\n|  3|Charlie|2024-03-01|    I|\n|  3|Charlie|2024-03-02|    J|\n|  3|Charlie|2024-03-03|    K|\n|  3|Charlie|2024-03-04|    L|\n|  4|  David|2024-04-01|    M|\n|  4|  David|2024-04-02|    N|\n|  4|  David|2024-04-03|    O|\n|  4|  David|2024-04-04|    P|\n|  5|   Emma|2024-05-01|    Q|\n|  5|   Emma|2024-05-02|    R|\n|  5|   Emma|2024-05-03|    S|\n|  5|   Emma|2024-05-04|    T|\n+---+-------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# initialize spark session\n",
    "spark = SparkSession.builder.appName(\"RemoveDuplicates\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, 'Alice', '2024-01-01', 'A'), (1, 'Alice', '2024-01-02', 'B'), (1, 'Alice', '2024-01-03', 'C'), (1, 'Alice', '2024-01-04', 'D'),\n",
    "    (2, 'Bob', '2024-02-01', 'E'), (2, 'Bob', '2024-02-02', 'F'), (2, 'Bob', '2024-02-03', 'G'), (2, 'Bob', '2024-02-04', 'H'),\n",
    "    (3, 'Charlie', '2024-03-01', 'I'), (3, 'Charlie', '2024-03-02', 'J'), (3, 'Charlie', '2024-03-03', 'K'), (3, 'Charlie', '2024-03-04', 'L'),\n",
    "    (4, 'David', '2024-04-01', 'M'), (4, 'David', '2024-04-02', 'N'), (4, 'David', '2024-04-03', 'O'), (4, 'David', '2024-04-04', 'P'),\n",
    "    (5, 'Emma', '2024-05-01', 'Q'), (5, 'Emma', '2024-05-02', 'R'), (5, 'Emma', '2024-05-03', 'S'), (5, 'Emma', '2024-05-04', 'T')\n",
    "]\n",
    "#Create Data Frame\n",
    "df = spark.createDataFrame(data, ['id', 'name', 'date', 'value'])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "626c414e-5da1-4710-b713-a710c6675fe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----+----+\n| id|   name|      date|value|rank|\n+---+-------+----------+-----+----+\n|  1|  Alice|2024-01-04|    D|   1|\n|  2|    Bob|2024-02-04|    H|   1|\n|  3|Charlie|2024-03-04|    L|   1|\n|  4|  David|2024-04-04|    P|   1|\n|  5|   Emma|2024-05-04|    T|   1|\n+---+-------+----------+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "#get the lastest date transaction for each customer\n",
    "\n",
    "part_cust = Window.partitionBy(\"name\").orderBy(col (\"date\").desc())\n",
    "df2 = df.withColumn(\"rank\",rank().over(part_cust))\n",
    "df2 = df2.filter(\"rank =1\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c226502-cec1-4797-a9e3-9dc3ad026628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|  AMT|\n+-----+\n|10000|\n|10000|\n| 5000|\n| 5000|\n| 5000|\n| 5000|\n| 3000|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# initialize spark session\n",
    " \n",
    "spark = SparkSession.builder.appName(\"CreateDataFrame\").getOrCreate() \n",
    "\n",
    "list1 = [\n",
    "    (10000, 'Alice', '2024-01-01', 'A'),\n",
    "    (10000, 'Alice', '2024-01-02', 'B'),\n",
    "    (5000, 'Bob', '2024-02-01', 'E'),\n",
    "    (5000, 'Bob', '2024-02-02', 'F'),\n",
    "    (5000, 'Bob', '2024-02-03', 'G'),\n",
    "    (5000, 'Bob', '2024-02-04', 'H'),\n",
    "    (3000, 'Charlie', '2024-03-01', 'I')\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(list1, ['id', 'name', 'date', 'value'])\n",
    "df =df.select(df.id.alias(\"AMT\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eac6bca-deb4-4773-8ba2-6ef2738af0fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>AMT</th><th>rowN</th><th>Rank</th><th>dense_rank</th></tr></thead><tbody><tr><td>10000</td><td>1</td><td>1</td><td>1</td></tr><tr><td>10000</td><td>2</td><td>1</td><td>1</td></tr><tr><td>5000</td><td>3</td><td>3</td><td>2</td></tr><tr><td>5000</td><td>4</td><td>3</td><td>2</td></tr><tr><td>5000</td><td>5</td><td>3</td><td>2</td></tr><tr><td>5000</td><td>6</td><td>3</td><td>2</td></tr><tr><td>3000</td><td>7</td><td>7</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10000,
         1,
         1,
         1
        ],
        [
         10000,
         2,
         1,
         1
        ],
        [
         5000,
         3,
         3,
         2
        ],
        [
         5000,
         4,
         3,
         2
        ],
        [
         5000,
         5,
         3,
         2
        ],
        [
         5000,
         6,
         3,
         2
        ],
        [
         3000,
         7,
         7,
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "AMT",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "rowN",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Rank",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dense_rank",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parnt_rank = Window.orderBy(col(\"AMT\").desc())\n",
    "df2 = df.withColumn(\"rowN\",row_number().over(parnt_rank)).withColumn(\"Rank\",rank().over(parnt_rank)).withColumn(\"dense_rank\",dense_rank().over(parnt_rank))\n",
    "display(df2)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Windows Functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}